{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8775270a-50bb-46ca-a4f9-28efb324e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import maxvit.models.hparams as hparams\n",
    "import maxvit.models.maxvit as layers\n",
    "\n",
    "import logging\n",
    "\n",
    "from zoobot.tensorflow.estimators import efficientnet_custom, efficientnet_standard, custom_layers\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "999dc66e-512f-444f-b4ff-7f73447d3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MaxViTTiny'\n",
    "config = hparams.lookup(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ab5ebed0-4305-46fe-8f30-87a2ed1e32aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__maxvit__': 1, 'dataset': 'imagenet', 'double_transpose': True, 'input': {'augname': 'randaug', 'ra_num_layers': 2, 'ra_magnitude': 15, 'mixup_alpha': 0.8, 'cutmix_alpha': 0.0, 'legacy_preprocess': True}, 'loss': {'xent_type': 'softmax', 'label_smoothing': 0.1}, 'train': {'split': None, 'image_size': 300, 'epochs': 300, 'batch_size': 4096, 'optimizer': 'adamw', 'lr_schedule': {'type': 'cosine', 'warmup_steps': 10000, 'warmup_epochs': None, 'lr_max': 0.003, 'lr_min': 1e-05}, 'weight_decay': 0.05, 'weight_decay_exclude': '.*(bias|scale|gain|gamma|beta).*', 'ema_decay': 0.9999, 'grad_clip': 1.0, 'steps': None}, 'eval': {'split': None, 'image_size': 224, 'batch_size': 16, 'steps': None}, 'path': {'ckpt_dir': None}, 'tpu': {'iterations_per_loop': 5000, 'save_checkpoints_steps': 5000, 'keep_checkpoint_max': 0, 'keep_checkpoint_every_n_hours': 4, 'use_bfloat16': True}, 'init': {'warm_start_mode': 'restore_train', 'warm_start_from': None}, 'model': {'block_type': ['maxvit', 'maxvit', 'maxvit', 'maxvit'], 'add_pos_enc': [False, False, False, False], 'downsample_loc': 'depth_conv', 'stem_hsize': [64, 64], 'num_blocks': [2, 2, 5, 2], 'hidden_size': [64, 128, 256, 512], 'window_size': 7, 'grid_size': 7, 'survival_prob': 0.8}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.train.image_size = 300\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "adb25e55-5cbd-4f24-a6f5-edd5b8ce84bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.eval.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "26ec1322-9d62-4d47-acc1-7a3d0307a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = layers.MaxViT(config.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e208e8-0d4e-409b-a4a6-11eaae1be67b",
   "metadata": {},
   "source": [
    "Try making the MaxVit plus keras blocks during the hack and check what it looks like.\n",
    "\n",
    "Make an if statement about using the head or not from the actual MaxViT data. Remove it in the MaxVIT code!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0c7cb-1b48-4dfd-9b45-26899721c620",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0c18e906-b446-48b6-8630-5b2dee78e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6595a825-a343-474e-a451-ea65789d36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7430ec31-423a-416f-b274-82e896bb73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.InputLayer(input_shape = input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5c4ccdba-97a6-488d-b7e1-e0a3a1a87d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = int(300 * 0.75)\n",
    "resize_size = 224\n",
    "always_augment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0c704ce6-bbd2-4070-b2df-c4ac2abc7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_augmentation_layers(\n",
    "    model,\n",
    "    crop_size=crop_size,\n",
    "    resize_size=resize_size,\n",
    "    always_augment=always_augment\n",
    "):\n",
    "    \"\"\"\n",
    "    Add image augmentation layers to end of ``model``.\n",
    "\n",
    "    The following augmentations are applied, in order:\n",
    "        - Random rotation (aliased)\n",
    "        - Random flip (horizontal and/or vertical)\n",
    "        - Random crop (not centered) down to ``(crop_size, crop_size)``\n",
    "        - Resize down to ``(resize_size, resize_size)``\n",
    "\n",
    "    If crop_size is within 10 of resize_size, resizing is skipped and instead the image is cropped directly to `resize_size`.\n",
    "    This is both faster and avoids information loss from aliasing.\n",
    "    I strongly suggest this approach if possible.\n",
    "\n",
    "    Model (probably tf.keras.Sequential) is modified in-place so this func. returns None.\n",
    "\n",
    "    TODO I would prefer to refactor this so augmentations are separate from the model, as with pytorch.\n",
    "    But it's not a high priority change.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): Model to add augmentation layers. Layers are added at *end*, so likely an empty model e.g. tf.keras.Sequential()\n",
    "        crop_size (int): desired length of image after random crop (assumed square)\n",
    "        resize_size (int): desired length of image after resizing (assumed square)\n",
    "        always_augment (bool, optional): If True, augmentations also happen at test time. Defaults to False.\n",
    "    \"\"\"\n",
    "    if crop_size < resize_size:\n",
    "        logging.warning('Crop size {} < final size {}, losing resolution'.format(\n",
    "            crop_size, resize_size))\n",
    "\n",
    "    resize = True\n",
    "    if np.abs(crop_size - resize_size) < 10:\n",
    "        logging.warning(\n",
    "            'Crop size and final size are similar: skipping resizing and cropping directly to resize_size (ignoring crop_size)')\n",
    "        resize = False\n",
    "        crop_size = resize_size\n",
    "\n",
    "    if always_augment:\n",
    "        rotation_layer = custom_layers.PermaRandomRotation\n",
    "        flip_layer = custom_layers.PermaRandomFlip\n",
    "        crop_layer = custom_layers.PermaRandomCrop\n",
    "    else:\n",
    "        rotation_layer = tf.keras.layers.experimental.preprocessing.RandomRotation\n",
    "        flip_layer = tf.keras.layers.experimental.preprocessing.RandomFlip\n",
    "        crop_layer = tf.keras.layers.experimental.preprocessing.RandomCrop\n",
    "\n",
    "\n",
    "    # np.pi fails with tf 2.5\n",
    "    model.add(rotation_layer(0.5, fill_mode='reflect'))  # rotation range +/- 0.25 * 2pi i.e. +/- 90*.\n",
    "    model.add(flip_layer())\n",
    "    model.add(crop_layer(crop_size, crop_size))\n",
    "    if resize:\n",
    "        logging.info('Using resizing, to {}'.format(resize_size))\n",
    "        model.add(tf.keras.layers.experimental.preprocessing.Resizing(\n",
    "            resize_size, resize_size, interpolation='bilinear'\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c7121b86-9ff5-47d7-8f74-da10cac7d1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Crop size and final size are similar: skipping resizing and cropping directly to resize_size (ignoring crop_size)\n"
     ]
    }
   ],
   "source": [
    "add_augmentation_layers(model, crop_size=crop_size, resize_size=resize_size,always_augment=always_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a2facd3d-f69a-4264-8218-77af4adb4d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__maxvit__': 1, 'dataset': 'imagenet', 'double_transpose': True, 'input': {'augname': 'randaug', 'ra_num_layers': 2, 'ra_magnitude': 15, 'mixup_alpha': 0.8, 'cutmix_alpha': 0.0, 'legacy_preprocess': True}, 'loss': {'xent_type': 'softmax', 'label_smoothing': 0.1}, 'train': {'split': None, 'image_size': 300, 'epochs': 300, 'batch_size': 4096, 'optimizer': 'adamw', 'lr_schedule': {'type': 'cosine', 'warmup_steps': 10000, 'warmup_epochs': None, 'lr_max': 0.003, 'lr_min': 1e-05}, 'weight_decay': 0.05, 'weight_decay_exclude': '.*(bias|scale|gain|gamma|beta).*', 'ema_decay': 0.9999, 'grad_clip': 1.0, 'steps': None}, 'eval': {'split': None, 'image_size': 224, 'batch_size': 16, 'steps': None}, 'path': {'ckpt_dir': None}, 'tpu': {'iterations_per_loop': 5000, 'save_checkpoints_steps': 5000, 'keep_checkpoint_max': 0, 'keep_checkpoint_every_n_hours': 4, 'use_bfloat16': True}, 'init': {'warm_start_mode': 'restore_train', 'warm_start_from': None}, 'model': {'block_type': ['maxvit', 'maxvit', 'maxvit', 'maxvit'], 'add_pos_enc': [False, False, False, False], 'downsample_loc': 'depth_conv', 'stem_hsize': [64, 64], 'num_blocks': [2, 2, 5, 2], 'hidden_size': [64, 128, 256, 512], 'window_size': 7, 'grid_size': 7, 'survival_prob': 0.8}}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b8ba443b-75b9-42b7-9cca-68d0f20df0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxvit_model = layers.MaxViT(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f4674a95-e975-421f-81f2-e2bd9cca22cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 128, 256, 512]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.model.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "48b472fd-3650-4dfd-9364-9dcb8ecf8844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__maxvit__': 1, 'dataset': 'imagenet', 'double_transpose': True, 'input': {'augname': 'randaug', 'ra_num_layers': 2, 'ra_magnitude': 15, 'mixup_alpha': 0.8, 'cutmix_alpha': 0.0, 'legacy_preprocess': True}, 'loss': {'xent_type': 'softmax', 'label_smoothing': 0.1}, 'train': {'split': None, 'image_size': 300, 'epochs': 300, 'batch_size': 4096, 'optimizer': 'adamw', 'lr_schedule': {'type': 'cosine', 'warmup_steps': 10000, 'warmup_epochs': None, 'lr_max': 0.003, 'lr_min': 1e-05}, 'weight_decay': 0.05, 'weight_decay_exclude': '.*(bias|scale|gain|gamma|beta).*', 'ema_decay': 0.9999, 'grad_clip': 1.0, 'steps': None}, 'eval': {'split': None, 'image_size': 224, 'batch_size': 16, 'steps': None}, 'path': {'ckpt_dir': None}, 'tpu': {'iterations_per_loop': 5000, 'save_checkpoints_steps': 5000, 'keep_checkpoint_max': 0, 'keep_checkpoint_every_n_hours': 4, 'use_bfloat16': True}, 'init': {'warm_start_mode': 'restore_train', 'warm_start_from': None}, 'model': {'block_type': ['maxvit', 'maxvit', 'maxvit', 'maxvit'], 'add_pos_enc': [False, False, False, False], 'downsample_loc': 'depth_conv', 'stem_hsize': [64, 64], 'num_blocks': [2, 2, 5, 2], 'hidden_size': [64, 128, 256, 512], 'window_size': 7, 'grid_size': 7, 'survival_prob': 0.8}}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "888ba5a4-bbdc-4b74-b35d-bde1a4fc5c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 5, 2]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ad5e8dd5-450f-4357-bb27-4ae20e4206d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnet_model = efficientnet_standard.EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a4fab9fc-ac31-46bd-b088-fe9d3824c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnet = efficientnet_custom.define_headless_efficientnet(  # from efficientnet_custom.py\n",
    "                                                                    # defines efficientnet model to train\n",
    "                                                                    # direct to maxvit_standard.py instead!\n",
    "            input_shape=(224,224,3),\n",
    "            get_effnet=effnet_model,  # model\n",
    "            # further kwargs will be passed to get_effnet\n",
    "            use_imagenet_weights=False,\n",
    "        )\n",
    "model.add(effnet)  # modify`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c22490f1-fe06-4d95-a59a-aeaa9429db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(custom_layers.PermaDropout(0.8, name='top_dropout'))\n",
    "efficientnet_custom.custom_top_dirichlet(model, 34)  # inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bf186f37-aa12-4a0e-a917-0451f3f693cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5031d955-682d-4529-bf8f-01e876b9dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build([300,300,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a7885d2d-84e8-4315-a554-dcaa983257fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " random_rotation_7 (RandomRo  (None, 300, 300, 3)      0         \n",
      " tation)                                                         \n",
      "                                                                 \n",
      " random_flip_7 (RandomFlip)  (None, 300, 300, 3)       0         \n",
      "                                                                 \n",
      " random_crop_7 (RandomCrop)  (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " sequential_12 (Sequential)  (None, 7, 7, 1280)        4049564   \n",
      "                                                                 \n",
      " global_average_pooling2d_3   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " top_dropout (PermaDropout)  (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 34)                43554     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,093,118\n",
      "Trainable params: 4,051,102\n",
      "Non-trainable params: 42,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408fa340-767a-4c8a-b8df-6c645f5923d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
